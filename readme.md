I hava finish the simple transform model,this achievement is very base.because Iuse ground floor api, not achieve GRU only by nn.GRU.

In this file ,I achieve GRU,scaleDotAttention,additiveAttention,and transform,and a attention visualize.

but transform may hava some wrong.So cant be use indirectly,But when you can't find GRU achievement, you can see this git.

我实现了一个GRU，不是直接调库的实现，是用底层的代码实现的。

除了GRU，还有神经网络Attention,scaleDotAttenton,还有最后的transform.

还有一个英文教程能帮你们理解这个东西，如果想学习的话，可以联系我微信：sgw951225,但实话说，要付费的，五块钱吧，都中国人，你懂的。如果需要写模型的话可以联系我微信。或者需要承接做深度学习，机器学习大作业的，都可以联系我，承接数据分析及自然语言处理业务。

tip:This is some useful resource about this task:
Attention is all you need:
http://blog.stupidme.me/transformer-attention-is-all-you-need/
https://yq.aliyun.com/articles/342508?utm_content=m_39938
https://zhuanlan.zhihu.com/p/47282410
构建模型的几种方法：
xbiquge.cc/book/42381/


使用技术：
torch.cat,bmm,matmul,cuda,tril,mask_fill_
anaconda导出库等对我新鲜的技术
 
